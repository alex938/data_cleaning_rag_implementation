# ============================================
# GLOBAL SETTINGS
# ============================================
customer_id: "customer1"
data_dir: "./data/customer1"
raw_dir: "./data/customer1/raw"              # Where you put original PDFs/DOCX/emails
processed_dir: "./data/customer1/processed"  # Where cleaned JSONL is stored

# ============================================
# STEP 1: step1_clean_data.py
# Document Processing Settings
# ============================================
document_processing:
  # Supported file types (add more as needed)
  supported_extensions:
    - ".pdf"
    - ".docx"
    - ".doc"
    - ".xlsx"
    - ".xls"
    - ".txt"
    - ".html"
    - ".eml"
    - ".msg"
  
  # Chunking strategy per document type
  chunking:
    # For large documents (PDFs, long reports)
    large_documents:
      extensions: [".pdf", ".docx", ".doc"]
      strategy: "recursive"  # Options: "recursive", "sentence", "fixed"
      chunk_size: 1000       # tokens/characters
      chunk_overlap: 200     # overlap between chunks
      min_chunk_size: 100    # discard smaller chunks
    
    # For medium documents (HTML pages, text files)
    medium_documents:
      extensions: [".html", ".txt"]
      strategy: "recursive"
      chunk_size: 800
      chunk_overlap: 150
      min_chunk_size: 50
    
    # For short documents (emails, messages)
    small_documents:
      extensions: [".eml", ".msg"]
      strategy: "none"       # Keep as single document
      chunk_size: null
      chunk_overlap: null
      min_chunk_size: null
    
    # For spreadsheets (extract per-sheet or per-row)
    spreadsheets:
      extensions: [".xlsx", ".xls"]
      strategy: "sheet"      # Treat each sheet as document
      chunk_size: null
      chunk_overlap: null
      min_chunk_size: null

  # Text cleaning options
  cleaning:
    remove_email_signatures: true
    remove_disclaimers: true
    remove_mobile_signatures: true
    normalize_whitespace: true
    max_consecutive_newlines: 2
    # Custom regex patterns to remove (per customer)
    custom_patterns:
      - "(?is)--+\\s*Original Message\\s*--+.*$"
      - "(?is)This e[- ]?mail.*?confidential.*?$"
      - "(?is)Sent from my (iPhone|Android).*?$"

# ============================================
# STEP 3: step3_create_embeddings_qdrant.py
# add_documents.py also uses these settings
# Embedding Configuration
# ============================================
embedding:
  model: "BAAI/bge-large-en-v1.5"
  # Alternative models based on use case:
  # - "sentence-transformers/all-MiniLM-L6-v2" (faster, smaller)
  # - "BAAI/bge-base-en-v1.5" (balanced)
  # - "intfloat/e5-large-v2" (high quality)
  
  batch_size: 32
  normalize_embeddings: true
  max_seq_length: 512  # Model's max token length
  
  # Device settings
  device: "cpu"  # Options: "cpu", "cuda", "mps"

# ============================================
# STEP 3 & 4: Qdrant Vector Database
# Used by: step3_create_embeddings_qdrant.py, step4_test_qdrant.py,
#          add_documents.py, query_engine.py
# ============================================
qdrant:
  host: "localhost"
  port: 6333
  collection: "customer1_docs"  # THIS IS WHERE YOUR DOCUMENTS ARE STORED
  
  # Vector configuration
  vector_config:
    distance: "Cosine"  # Options: "Cosine", "Euclidean", "Dot"
  
  # Indexing options (for performance)
  indexing:
    hnsw_config:
      m: 16                    # Number of edges per node
      ef_construct: 100        # Construction time parameter
    quantization: null         # Options: null, "scalar", "product"

# ============================================
# query_engine.py - Retrieval Settings
# ============================================
query:
  # Retrieval settings
  top_k: 5                     # Number of documents to retrieve
  score_threshold: 0.7         # Minimum similarity score
  
  # Context window for LLM
  max_context_tokens: 3000     # Total tokens to send to LLM

# ============================================
# query_engine.py - Ollama LLM Configuration
# ============================================
llm:
  host: "http://192.168.2.102:11434"  # Your Ollama server
  model: "llama3.2"                    # Available: llama3.2, mistral, qwen2.5, etc.
  temperature: 0.3                     # Lower = more focused, higher = more creative
  max_tokens: 2000                     # Maximum response length

# ============================================
# Logging & Monitoring
# ============================================
logging:
  level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  log_file: "./logs/pipeline.log"
  
monitoring:
  track_processing_time: true
  track_chunk_statistics: true
  track_embedding_quality: false
